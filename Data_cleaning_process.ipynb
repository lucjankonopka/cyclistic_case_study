{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Process \n",
    "\n",
    "Source file: *cyclistic_data_cleaning.py*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading pandas library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt2021 = pd.read_csv('workfiles/202110-divvy-tripdata.csv', index_col=0)\n",
    "nov2021 = pd.read_csv('workfiles/202111-divvy-tripdata.csv', index_col=0)\n",
    "dec2021 = pd.read_csv('workfiles/202112-divvy-tripdata.csv', index_col=0)\n",
    "jan2022 = pd.read_csv('workfiles/202201-divvy-tripdata.csv', index_col=0)\n",
    "feb2022 = pd.read_csv('workfiles/202202-divvy-tripdata.csv', index_col=0)\n",
    "mar2022 = pd.read_csv('workfiles/202203-divvy-tripdata.csv', index_col=0)\n",
    "apr2022 = pd.read_csv('workfiles/202204-divvy-tripdata.csv', index_col=0)\n",
    "may2022 = pd.read_csv('workfiles/202205-divvy-tripdata.csv', index_col=0)\n",
    "jun2022 = pd.read_csv('workfiles/202206-divvy-tripdata.csv', index_col=0)\n",
    "jul2022 = pd.read_csv('workfiles/202207-divvy-tripdata.csv', index_col=0)\n",
    "aug2022 = pd.read_csv('workfiles/202208-divvy-tripdata.csv', index_col=0)\n",
    "sep2022 = pd.read_csv('workfiles/202209-divvy-tripdata.csv', index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging all datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all datasets into one:\n",
    "dataframe = [okt2021, nov2021, dec2021, jan2022, feb2022,\n",
    "             mar2022, apr2022, may2022, jun2022, jul2022, aug2022, sep2022]\n",
    "df_complete = pd.concat(dataframe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the structure of all dataset files:\n",
    "All tables should have the same structure to complete the merging step successfully. Check\n",
    "showed that all 12 datasets have a countable number of rows and 12 columns with the same\n",
    "index names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking number and columns and its names in datasets:\n",
    "for _ in dataframe:\n",
    "    print(_.shape)\n",
    "    print(_.columns)\n",
    "# Raw datasets contain 12 same columns. Tables are consistent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the structure of the merged dataset:\n",
    "Merged table keeps the structure of the single raw dataset. It has 5.828.235 rows and 12\n",
    "columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete.shape\n",
    "df_complete.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pushing “ride_id” index at the beginning of the table:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_complete = df_complete.reset_index(level=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Changing data types to datetype format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns \"started_at\" and \"ended_at\" should be in datetype format:\n",
    "df_complete['started_at'] = pd.to_datetime(df_complete['started_at'])\n",
    "df_complete['ended_at'] = pd.to_datetime(df_complete['ended_at'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Looking for “typos” or “misspellings” in the whole set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete.rideable_type.unique()\n",
    "# There are 3 types of bikes: \"electric bike\", \"docked bike\" and \"classic bike\". No errors.\n",
    "df_complete.member_casual.unique()\n",
    "# There are 2 types of users: \"casual\", \"member\". No errors.\n",
    "df_complete.start_station_id.value_counts()\n",
    "df_complete.start_station_name.value_counts()\n",
    "df_complete.end_station_id.value_counts()\n",
    "df_complete.end_station_name.value_counts()\n",
    "# There are many names and id's that occur just once. \n",
    "# They could be error inputs or new/deleted stations. To further investigation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No such errors were found.\n",
    "There are many station names and id's that occur just once. They could be error inputs or\n",
    "new/deleted stations. This problem has been left at this point to eventual further\n",
    "investigation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking length of the ride id’s and looking for its duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking length of the id's:\n",
    "df_complete['ride_id'].map(len).unique()\n",
    "# All id's have 16 characters\n",
    "\n",
    "# Checking for duplicate inputs based on ride id:\n",
    "df_complete.duplicated(subset=['ride_id']).value_counts()\n",
    "# There are no duplicates of ride ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for empty cells, null values in single columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['ride_id'].isnull().value_counts()\n",
    "df_complete['rideable_type'].isnull().value_counts()\n",
    "df_complete['started_at'].isnull().value_counts()\n",
    "df_complete['ended_at'].isnull().value_counts()\n",
    "# No empty cells or null values\n",
    "\n",
    "df_complete['start_station_name'].isnull().value_counts()\n",
    "df_complete['start_station_id'].isnull().value_counts()\n",
    "df_complete['start_station_name'].isnull().value_counts(\n",
    ") & df_complete['start_station_id'].isnull().value_counts()\n",
    "# 895032 rows with empty cells or null values in start station description columns\n",
    "# start_station_id inconsistent length and format\n",
    "\n",
    "df_complete['end_station_name'].isnull().value_counts()\n",
    "df_complete['end_station_id'].isnull().value_counts()\n",
    "df_complete['end_station_name'].isnull().value_counts(\n",
    ") & df_complete['end_station_id'].isnull().value_counts()\n",
    "# 958227 rows with empty cells or null values in start station description columns\n",
    "# end_station_id inconsistent length and format\n",
    "\n",
    "df_complete['start_station_name'].isnull().value_counts(\n",
    ") & df_complete['end_station_name'].isnull().value_counts()\n",
    "# 821264 rows where both start and end station is not given\n",
    "\n",
    "df_complete['start_lat'].isnull().value_counts()\n",
    "df_complete['start_lng'].isnull().value_counts()\n",
    "# No empty cells or null values\n",
    "\n",
    "df_complete['end_lat'].isnull().value_counts()\n",
    "df_complete['end_lng'].isnull().value_counts()\n",
    "# 5844 rows with empty cells or null values in end coordinates columns\n",
    "\n",
    "df_complete['member_casual'].isnull().value_counts()\n",
    "# No empty cells or null values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were few inputs without recorded station description (names or id’s) or coordinates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing incomplete instances and checking the number of deleted rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting rows with missing data:\n",
    "no_nan_data = df_complete.dropna()\n",
    "no_nan_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset without NAN values contains 4474141 rows which is 76% of the whole set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking for unwanted data – test data:\n",
    "There were few instances with “TEST” in start and end station id’s columns. All of them should not be considered. After deletion there were 4472680 rows left (about 75% of the whole\n",
    "dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for the unwanted data - test data.\n",
    "# Context: TEST found in one of the start stations id's \"Hubbard Bike-checking (LBS-WH-TEST)\"\n",
    "no_nan_data['start_station_id'].str.contains('TEST').value_counts()\n",
    "no_nan_data['start_station_name'].str.contains('TEST').value_counts()\n",
    "# 1207 test rides found in start station id's. All should not be considered:\n",
    "# Deleting test rides:\n",
    "no_test_start_data = no_nan_data[no_nan_data['start_station_id'].str.contains(\n",
    "    'TEST') != True]\n",
    "\n",
    "no_test_start_data['end_station_id'].str.contains('TEST').value_counts()\n",
    "no_test_start_data['end_station_name'].str.contains('TEST').value_counts()\n",
    "# 254 test rides found in rest end station id's. All should not be considered:\n",
    "# Deleting test rides:\n",
    "no_nan_no_test_data = no_test_start_data[no_test_start_data['end_station_id'].str.contains(\n",
    "    'TEST') != True]\n",
    "# 4472680 rows left"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data:\n",
    "#### Adding new column to calculate ride time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with no test rides and no NaN values:\n",
    "no_nan_no_test_data.head()\n",
    "\n",
    "# Creating copy of the dataframe to add new column:\n",
    "df_ridetime = no_nan_no_test_data.copy(deep=True)\n",
    "\n",
    "# Adding new column with ride time:\n",
    "df_ridetime.insert(\n",
    "    loc=4, column='ride_time[s]', value=df_ridetime['ended_at'] - df_ridetime['started_at'])\n",
    "# Changing ride time to seconds:\n",
    "df_ridetime['ride_time[s]'] = df_ridetime['ride_time[s]'].astype(\n",
    "    'timedelta64[s]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All time values were calculated into seconds for further analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for negative and irrelevant ride times:\n",
    "Negative ride time value means an input error which should not be considered. For the\n",
    "analysis purpose also the ride time below 60 seconds won’t be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting data by ride time:\n",
    "df_rt_sorted = df_ridetime.sort_values(by='ride_time[s]')\n",
    "\n",
    "# Checking for negative ride time values:\n",
    "df_rt_sorted[df_rt_sorted['ride_time[s]'] < 0]\n",
    "df_rt_sorted[df_rt_sorted['started_at'] > df_rt_sorted['ended_at']]\n",
    "# 71 wrong inputs with negative ride times. Deleting:\n",
    "df_rt_no_neg = df_rt_sorted[df_rt_sorted['ride_time[s]'] > 0]\n",
    "# Checking for the low ride time values, assuming these are incorrect or irrelevant records (below 60s):\n",
    "df_rt_no_neg[df_rt_no_neg['ride_time[s]'] < 60]\n",
    "# There are 73439 inputs with ride time below 60s. Deleting:\n",
    "df_rt_cleaned = df_rt_sorted[df_rt_sorted['ride_time[s]'] >= 60]\n",
    "\n",
    "# Data sorted by date:\n",
    "df_sort_date = df_rt_cleaned.sort_values(by=\"started_at\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting irrelevant columns such as ride id’s, station id’s and station coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the analysis purposes ride id's, station id's and coordinates are irrelevant.\n",
    "df_sort_date.columns\n",
    "columns_to_be_dropped = ['ride_id', 'start_station_id',\n",
    "                         'end_station_id', 'start_lat', 'start_lng', 'end_lat', 'end_lng']\n",
    "df_drop_columns = df_sort_date.drop(columns=columns_to_be_dropped)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding columns with the day of the week for each ride (0 – Monday, 6 – Sunday):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding columns with the day of the week that each ride started\n",
    "# 0 - Monday, 6 - Sunday:\n",
    "df_drop_columns.insert(loc=4, column='weekday', value=df_ridetime['started_at'].dt.weekday)\n",
    "\n",
    "all_rides = df_drop_columns.copy(deep=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new column names for cleaned dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cleaned_rides_new_columns = all_rides.copy()\n",
    "\n",
    "all_cleaned_rides_new_columns.columns = ['BikeType',\n",
    "                                        'RideStart',\n",
    "                                        'RideEnd',\n",
    "                                        'RideTime[s]',\n",
    "                                        'Weekday',\n",
    "                                        'StartStation',\n",
    "                                        'EndStation',\n",
    "                                        'UserType']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting clean dataset, ready for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cleaned_rides_new_columns.to_csv('cleaned_data/cyclistic_202110-202209_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cef1d38ca744d99e247d6c4d165e12a9aa3f29f1e83d9722af487dd60dc4e580"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
